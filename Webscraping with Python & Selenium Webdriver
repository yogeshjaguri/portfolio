import time
import selenium
from selenium import webdriver # allow launching browser
from selenium.webdriver.common.by import By # allow search with parameters
from selenium.webdriver.support.ui import WebDriverWait # allow waiting for page to load
from selenium.webdriver.support import expected_conditions as EC # determine whether the web page has loaded
from selenium.common.exceptions import TimeoutException # handling timeout situation
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

driver_option = webdriver.ChromeOptions()
driver_option.add_argument(" â€” incognito")
# Change this to your own chromedriver path!
def create_webdriver():
 return webdriver.Chrome(executable_path=ChromeDriverManager().install())

# Open the website
browser = create_webdriver()
browser.get("https://github.com/collections/machine-learning")
browser.maximize_window()

# Extract all projects
projects = browser.find_element(By.XPATH, "//div[@class='pt-md-6 pb-3 pb-md-5']")
time.sleep(5)

# Extract information for each project
project_list = {}
for proj in projects:
 proj_name = proj.text # Project name
 proj_url = proj.find_elements(By.XPATH, "//div[@class='pt-md-6 pb-3 pb-md-5']").get_attribute('href')  # Project URL
 project_list[proj_name] = proj_url

 # Close connection
browser.quit()

# Extracting data
project_df = pd.DataFrame.from_dict(project_list, orient = 'index')


# Manipulate the table
project_df['project_name'] = project_df.index
project_df.columns = ['project_url', 'project_name']
project_df = project_df.reset_index(drop=True)


# Export project dataframe to CSV
project_df.to_csv('project_list.csv')

from concurrent.futures import ProcessPoolExecutor
import concurrent.futures

def scrape_url(url):
 new_browser = create_webdriver()
 new_browser.get(url)
 
 # Extract required data here
 # ...
 
 new_browser.quit()
 
 return data

with ProcessPoolExecutor(max_workers=4) as executor:
 future_results = {executor.submit(scrape_url, url) for url in urlarray}

results = []
for future in concurrent.futures.as_completed(future_results):
 results.append(future.result())
